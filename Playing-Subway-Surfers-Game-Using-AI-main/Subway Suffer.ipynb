{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center><font style=\"color:rgb(100,109,254)\">Playing Subway Surfers Game using Pose Detection</font> </center>**\n",
    "\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1Msiu4noiq5NKViqXX8TE-6sei6ycS1Xx'>\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1bREfnsfCWjVyMRjXM0kI0V33kRQ7f_dY'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform Pose Detection\n",
    "\n",
    "-  Control Keyboard and Mouse with PyautoGUI\n",
    "\n",
    "-  Build the Final Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\"> Import the Libraries</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pyautogui\n",
    "from time import time\n",
    "from math import hypot\n",
    "#Multidimensional Euclidean distance from the origin to a point.\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "\n",
    "# Potential of MediaPipe Machine Learning for Advanced Computer Vision Applications\n",
    "# building real-time perception pipelines in various applications, including computer vision,machine learning, and multimedia analysis.\n",
    "# DL mode ha  , hand dection and face dection\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(134,19,348)\">Initialize the Pose Detection Model</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize mediapipe pose class\n",
    "mp_pose=mp.solutions.pose\n",
    "\n",
    "#setup with image\n",
    "pose_image=mp_pose.Pose(static_image_mode=True,min_detection_confidence=0.5,model_complexity=1)\n",
    "\n",
    "#set up for video\n",
    "pose_video=mp_pose.Pose(static_image_mode=False,model_complexity=1,min_detection_confidence=0.7,\n",
    "                        min_tracking_confidence=0.7)\n",
    "\n",
    "#for drawing the line\n",
    "mp_drawing=mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Perform Pose Detection</font>**\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=download&id=1CDO0KiXZEOuWc7xLEm7EFLLQf2hydCoI\">\n",
    "\n",
    "if another will come into the frame it will ignore it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(134,19,348)\">i am not display the image becouse it consume more resource\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPose(image,pose, draw=False,display=False):\n",
    "    \n",
    "    # copy of input image\n",
    "    output_image=image.copy()\n",
    "\n",
    "    #bgr to rgb\n",
    "    imageRGB=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # deceted the pose\n",
    "    results =pose.process(imageRGB)  # ya rgb image sa coordinate nikal ga\n",
    "     #or coordinate wapasw sa image ko dana hoga draw karna ka leya\n",
    "\n",
    "\n",
    "    # draw the land mark\n",
    "    if results.pose_landmarks and draw:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=output_image, landmark_list=results.pose_landmarks,\n",
    "            connections=mp_pose.POSE_CONNECTIONS, # mp_pose uupar define ha\n",
    "            landmark_drawing_spec=mp_drawing.DrawingSpec( color=(255,255,255),\n",
    "                                                         thickness=3, circle_radius=3\n",
    "                                                         ),    # or mp_drawing uupar definr ha#for drawing the linn\n",
    "            connection_drawing_spec=mp_drawing.DrawingSpec(color=(49,125,237),\n",
    "                                                            thickness=2, \n",
    "                                                            circle_radius=2\n",
    "                                                            )                                                        \n",
    "                                )\n",
    "    #for checking the original input image and  resultant\n",
    "    if display:\n",
    "        # Display the original input image and the resultant image.\n",
    "        pass\n",
    "\n",
    "\n",
    "        # plt.figure(figsize=[22,22])\n",
    "        # plt.subplot(121)\n",
    "        # plt.imshow(image[:,:,::-1])\n",
    "        # plt.title(\"Original Image\");plt.axis('off');\n",
    "        # plt.subplot(122);\n",
    "        # plt.imshow(output_image[:,:,::-1]);\n",
    "        # plt.title(\"Output Image\");plt.axis('off');\n",
    "\n",
    "\n",
    "    else:\n",
    "# Return the output image and the results of pose landmarks detection.\n",
    "        return output_image , results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">test the decuted function</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not compulsery to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image=cv2.imread(\"C:\\\\Users\\\\ishuk\\\\Desktop\\\\final project on summer intership in 3rd year\\\\ishu.jpg\")\n",
    "# output_image = image.copy()\n",
    "# image  = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "# result = pose_image.process(image)\n",
    "# result.pose_landmarks  \n",
    "\n",
    "# mp_drawing.draw_landmarks(image=output_image, landmark_list=result.pose_landmarks,\n",
    "#                                   connections=mp_pose.POSE_CONNECTIONS,\n",
    "#                                   landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,255,255),\n",
    "#                                                                                thickness=3, circle_radius=3),\n",
    "#                                   connection_drawing_spec=mp_drawing.DrawingSpec(color=(49,125,237),\n",
    "#                                                                                thickness=2, circle_radius=2))\n",
    "\n",
    "# plt.figure(figsize=[22,22])\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(output_image[:,:,::-1])\n",
    "# plt.title(\"Original Image\");plt.axis('off');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Control Starting Mechanism</font>**\n",
    "\n",
    "# hand joint game start\n",
    "# move left so it will move left\n",
    "# move right so it will move right\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1p76mydN2UXU_0lMpQD5pzyM01ec2PQDy' width=300>\n",
    "\n",
    "utilize an appropriate threshold value to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkHandsJoined(image,results,draw=False,desplay=False):\n",
    "    height , width, _ =image.shape\n",
    "    # copy of image\n",
    "    output_image= image.copy()\n",
    "\n",
    "    # getting lest wtrist landmark x and y coordinate\n",
    "    left_wrist_landmark = (results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST].x * width,\n",
    "                          results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_WRIST].y * height)\n",
    "\n",
    "    # Get the right wrist landmark x and y coordinates.\n",
    "    right_wrist_landmark = (results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST].x * width,\n",
    "                           results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_WRIST].y * height)\n",
    "    \n",
    "    # ecular distance\n",
    "    euclidean_distance=int(hypot\n",
    "                           (\n",
    "                               left_wrist_landmark[0]-right_wrist_landmark[0],\n",
    "                            left_wrist_landmark[1]-right_wrist_landmark[1]\n",
    "                            )\n",
    "                            )\n",
    "     # Compare the distance\n",
    "    if euclidean_distance <130:\n",
    "       \n",
    "       #set hand join\n",
    "       hand_status='Hands Joined'\n",
    "       color=(0,255,0)\n",
    "\n",
    "       #otherwise\n",
    "    else:\n",
    "        hand_status='Hands Not Joined'\n",
    "        color=(0,255,0)\n",
    "    #hands joined status and hand distance\n",
    "    if draw:\n",
    "        cv2.putText(output_image, hand_status,(10,30),cv2.FONT_HERSHEY_PLAIN, 2, color ,3)\n",
    "\n",
    "        # write the equiler distance\n",
    "        cv2.putText(\n",
    "            output_image, f'Distance: {euclidean_distance}' , (10,70) ,\n",
    "            cv2.FONT_HERSHEY_PLAIN , 2, color ,3\n",
    "        )\n",
    "    #cheak if the output image is specified to be display\n",
    "\n",
    "    if display:\n",
    "        #display the output image\n",
    "        pass\n",
    "\n",
    "        # plt.figure(figsize=[10,10])\n",
    "        # plt.imshow(output_image[:,:,::-1])\n",
    "        # plt.title(\"Output Image\");plt.axis('off');\n",
    "    #other wise\n",
    "    else:\n",
    "        return output_image , hand_status\n",
    "    #side sa add karna para retyurn statement\n",
    "    return output_image , hand_status\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">cheak the function checkHandsJoined() work in real life</font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera_video=cv2.VideoCapture(0)\n",
    "# camera_video.set(3,1280)\n",
    "# camera_video.set(4,960)\n",
    "\n",
    "# #window creat\n",
    "# cv2.namedWindow('Hands Joinde?' , cv2.WINDOW_NORMAL)\n",
    "\n",
    "# while camera_video.isOpened():\n",
    "#     ok , frame=camera_video.read()\n",
    "\n",
    "#     if not ok:\n",
    "#         continue\n",
    "\n",
    "#     frame=cv2.flip(frame, 1)\n",
    "#     # get the hight and width\n",
    "#     frame_height, frame_width, _ = frame.shape\n",
    "\n",
    "#     # pose detection on the frame.\n",
    "#     frame, results= detectPose(frame,pose_video,draw=True)\n",
    "#     #cheak the postion landmark and frame\n",
    "#     if results.pose_landmarks:\n",
    "#         # cheak the left right hand joins\n",
    "#         frame, _ = checkHandsJoined(frame, results, draw=True)\n",
    "\n",
    "\n",
    "#     #display the frame\n",
    "#     cv2.imshow('Hands Joined?' ,frame)\n",
    "#     #wait key\n",
    "#     k=cv2.waitKey(1) & 0xFF\n",
    "#     # Check if 'ESC' is pressed and break the loop.\n",
    "\n",
    "#     if k==ord('x'):\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "# # Release the VideoCapture Object and close the windows.\n",
    "# camera_video.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Control Horizontal Movements</font>**\n",
    "\n",
    "- implementation of the left and right movements control\n",
    "- create a function `checkLeftRight()` that will take in the pose detection results returned by detectPose()\n",
    "- will use the x-coordinates of the RIGHT_SHOULDER and LEFT_SHOULDER\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1LhngpRrIJYMYIlKMnUep4YxdXMni3RvI'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkLeftRight(image, results, draw=False, display=False):\n",
    "    # horizontal position (left ,right and center) of persion\n",
    "\n",
    "    horizontal_position=None\n",
    "    #get hight and width of image\n",
    "    height, width,_=image.shape\n",
    "    output_image=image.copy()\n",
    "      # Retreive the x-coordinate of the left shoulder landmark.\n",
    "    left_x = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].x * width)\n",
    "\n",
    "    # Retreive the x-corrdinate of the right shoulder landmark.\n",
    "    right_x = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].x * width)\n",
    "    # if persion at left then x-coordinate is less than than or equal to the x-corrdinate of the center of the image.\n",
    "    if (right_x <= width//2 and left_x <= width//2):\n",
    "            \n",
    "        # Set the person's position to left.\n",
    "        horizontal_position = 'Left'\n",
    "\n",
    "    # Same as right\n",
    "    elif (right_x >= width//2 and left_x >= width//2):\n",
    "        \n",
    "        # Set the person's position to right.\n",
    "        horizontal_position = 'Right'\n",
    "    \n",
    "    # Check if the person is at center that is when right shoulder landmark x-corrdinate is greater than or equal to\n",
    "    # and left shoulder landmark x-corrdinate is less than or equal to the x-corrdinate of the center of the image.\n",
    "    elif (right_x >= width//2 and left_x <= width//2):\n",
    "        \n",
    "        # Set the person's position to center.\n",
    "        horizontal_position = 'Center'\n",
    "\n",
    "    #cheak if the person horizontal postion and line and ceter of image\n",
    "\n",
    "    if draw:\n",
    "        # Write the horizontal position of the person on the image\n",
    "        cv2.putText(output_image, horizontal_position, (5, height - 10), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    " # Draw a line at the center of the image.\n",
    "        cv2.line(output_image, (width//2, 0), (width//2, height), (255, 255, 255), 2)\n",
    "    if display:\n",
    "    \n",
    "        # Display the output image.\n",
    "        pass\n",
    "        # plt.figure(figsize=[10,10])\n",
    "        # plt.imshow(output_image[:,:,::-1]);\n",
    "        # plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "    \n",
    "        # Return the output image and the person's horizontal position.\n",
    "        return output_image, horizontal_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">test the checkLeftRight()</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera_video = cv2.VideoCapture(0)\n",
    "# camera_video.set(3,1280)\n",
    "# camera_video.set(4,960)\n",
    "# cv2.namedWindow('Horizontal Movements', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# # Iterate until the webcam is accessed successfully.\n",
    "# while camera_video.isOpened():\n",
    "    \n",
    "#     # Read a frame.\n",
    "#     ok, frame = camera_video.read()\n",
    "    \n",
    "#     if not ok:\n",
    "#         continue\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "\n",
    "#     frame_height, frame_width, _ = frame.shape\n",
    "#     # Perform the pose detection on the frame.\n",
    "#     frame, results = detectPose(frame, pose_video, draw=True)\n",
    "#     # Check if the pose landmarks in the frame are detected.\n",
    "\n",
    "#     if results.pose_landmarks:\n",
    "#         frame, _= checkLeftRight(frame, results,draw=True)\n",
    "        \n",
    "#     # Display the frame.\n",
    "#     cv2.imshow('Horizontal Movements', frame)\n",
    "    \n",
    "#     k=cv2.waitKey(1) & 0xFF\n",
    "#     # Check if 'ESC' is pressed and break the loop.\n",
    "\n",
    "#     if k==ord('x'):\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "# # Release the VideoCapture Object and close the windows.\n",
    "# camera_video.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **<font style=\"color:rgb(134,19,348)\">Control Vertical Movements</font>**\n",
    "\n",
    "- jump and down\n",
    "- create checkJumpCrouch() that will check whether the posture  `Jumping Crouching Standing` by the function detectPose()\n",
    "- checkJumpCrouch()retrieve the RIGHT_SHOULDER and  LEFT_SHOULDER landmarks of y-coordinate \n",
    "- The threshold (**`MID_Y`**) will be the approximate y-coordinate of the midpoint of both shoulders of the person while in standing posture. It will be calculated before starting the game in the  Final Applicatio and will be passed to the function **`checkJumpCrouch()`**. \n",
    "\n",
    "<img src='https://drive.google.com/uc?export=download&id=1fNAsoK964C4ASIkX6UXJNtvooZjlZNQT'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkJumpCrouch(image, results,MID_Y=250, draw=False):\n",
    "    #get hight and width of images\n",
    "\n",
    "    height, width , _ =image.shape\n",
    "    # create a copy of image\n",
    "    output_image=image.copy()\n",
    "    # y-coordinate of the left shoulder landmark.\n",
    "    left_y= int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].y * height)\n",
    "    # y-coordinate of the right shoulder landmark.\n",
    "    right_y = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y * height)\n",
    "\n",
    "    # Calculate the y-coordinate of the mid-point of both shoulders.\n",
    "    actual_mid_y = abs(right_y + left_y) // 2\n",
    "    #calculating the upper and lower bond of the therosold\n",
    "    lower_bound= MID_Y -20\n",
    "    upper_bound = MID_Y + 100\n",
    "       \n",
    "    # Check if the person has jumped that is when the y-coordinate of the mid-point \n",
    "    # of both shoulders is less than the lower bound.\n",
    "    if (actual_mid_y < lower_bound):\n",
    "        \n",
    "        # Set the posture to jumping.\n",
    "        posture = 'Jumping'\n",
    "    \n",
    "    # Check if the person has crouched that is when the y-coordinate of the mid-point \n",
    "    # of both shoulders is greater than the upper bound.\n",
    "    elif (actual_mid_y > upper_bound):\n",
    "        \n",
    "        # Set the posture to crouching.\n",
    "        posture = 'Crouching'\n",
    "    \n",
    "    # Otherwise the person is standing and the y-coordinate of the mid-point \n",
    "    # of both shoulders is between the upper and lower bounds.    \n",
    "    else:\n",
    "        \n",
    "        # Set the posture to Standing straight.\n",
    "        posture = 'Standing'\n",
    "    if draw:\n",
    "    \n",
    "        # Write the posture of the person on the image. \n",
    "        cv2.putText(output_image, posture, (5, height - 50), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 3)\n",
    "        \n",
    "        # Draw a line at the intial center y-coordinate of the person (threshold).\n",
    "        cv2.line(output_image, (0, MID_Y),(width, MID_Y),(255, 255, 255), 2)\n",
    "    # Check if the output image is specified to be displayed.\n",
    "    if display:\n",
    "\n",
    "        # Display the output image.\n",
    "        pass\n",
    "        # plt.figure(figsize=[10,10])\n",
    "        # plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "    \n",
    "        # Return the output image and posture indicating whether the person is standing straight or has jumped, or crouched.\n",
    "        return output_image, posture\n",
    "    #retur will add by me\n",
    "    return output_image, posture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">test the function</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera_video = cv2.VideoCapture(0)\n",
    "# camera_video.set(3,1280)\n",
    "# camera_video.set(4,960)\n",
    "\n",
    "# # Create named window for resizing purposes.\n",
    "# cv2.namedWindow('Verticial Movements', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# # Iterate until the webcam is accessed successfully.\n",
    "# while camera_video.isOpened():\n",
    "    \n",
    "#     # Read a frame.\n",
    "#     ok, frame = camera_video.read()\n",
    "    \n",
    "#     # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "#     if not ok:\n",
    "#         continue\n",
    "    \n",
    "#     # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     # Get the height and width of the frame of the webcam video.\n",
    "#     frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "#     # Perform the pose detection on the frame.\n",
    "#     frame, results = detectPose(frame, pose_video, draw=True)\n",
    "    \n",
    "#     # Check if the pose landmarks in the frame are detected.\n",
    "#     if results.pose_landmarks:\n",
    "            \n",
    "#         # Check the posture (jumping, crouching or standing) of the person in the frame. \n",
    "#         frame, _ = checkJumpCrouch(frame, results, draw=True)\n",
    "            \n",
    "#     cv2.imshow('Verticial Movements', frame)\n",
    "\n",
    "#     k=cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "\n",
    "#     if k==ord('x'):\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "# # Release the VideoCapture Object and close the windows.\n",
    "# camera_video.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\"> Control Keyboard and Mouse with PyautoGUI</font>**\n",
    "\n",
    "- to trigger the required keyboard keypress events, depending upon the output of the functions created above.\n",
    "\n",
    "- pyAutoGUI allows you to easily control the mouse and keyboard event through scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how simple it is to trigger the **`up`** arrow keypress event using pyautogui.\n",
    "\n",
    "##Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press the up key.\n",
    "pyautogui.press(keys='down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press the up (4 times) and down (1 time) key.\n",
    "pyautogui.press(keys=['up', 'up', 'up', 'up', 'down'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`pyautogui.keyDown(key)`**: Presses and holds down the specified `key`.\n",
    "\n",
    "* **`pyautogui.keyUp(key)`**:   Releases up the specified `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hold down the shift key\n",
    "# pyautogui.keyDown(key='shift')\n",
    "\n",
    "# #press enter 2 time\n",
    "# pyautogui.press(key='enter', presses=2)\n",
    "\n",
    "# # realeased the shift key\n",
    "# pyautogui.keyUp(key='shift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will run automatically due to keypress events in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "print('Hello!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Press the mouse right button.\n",
    "pyautogui.click(button='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\">Step 6: Build the Final Application</font>**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<font style=\"color:rgb(134,19,348)\"> we Have Building big </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ishuk\\Desktop\\final project on summer intership in 3rd year\\Subway Suffer.ipynb Cell 36\u001b[0m line \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m  frame_height, frame_width, _ \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mshape\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m  \u001b[39m# Perform the pose detection on the frame.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m  frame, results \u001b[39m=\u001b[39m detectPose(frame, pose_video, draw\u001b[39m=\u001b[39;49mgame_started)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# cheaking the landmarks in frames\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m  \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:\n",
      "\u001b[1;32mc:\\Users\\ishuk\\Desktop\\final project on summer intership in 3rd year\\Subway Suffer.ipynb Cell 36\u001b[0m line \u001b[0;36mdetectPose\u001b[1;34m(image, pose, draw, display)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m imageRGB\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mcvtColor(image,cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# deceted the pose\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m results \u001b[39m=\u001b[39mpose\u001b[39m.\u001b[39;49mprocess(imageRGB)  \u001b[39m# ya rgb image sa coordinate nikal ga\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m  \u001b[39m#or coordinate wapasw sa image ko dana hoga draw karna ka leya\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# draw the land mark\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ishuk/Desktop/final%20project%20on%20summer%20intership%20in%203rd%20year/Subway%20Suffer.ipynb#X50sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks \u001b[39mand\u001b[39;00m draw:\n",
      "File \u001b[1;32mc:\\Users\\ishuk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m   \u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n\u001b[0;32m    186\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ishuk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Subway Surfers with Pose Detection', cv2.WINDOW_NORMAL)\n",
    " \n",
    "# Initialize a variable to store the time of the previous frame.\n",
    "time1 = 0\n",
    "\n",
    "# Initialize a variable to store the state of the game (started or not).\n",
    "game_started = False   \n",
    "\n",
    "# Initialize a variable to store the index of the current horizontal position of the person.\n",
    "# At Start the character is at center so the index is 1 and it can move left (value 0) and right (value 2).\n",
    "x_pos_index = 1\n",
    "\n",
    "# Initialize a variable to store the index of the current vertical posture of the person.\n",
    "# At Start the person is standing so the index is 1 and he can crouch (value 0) and jump (value 2).\n",
    "y_pos_index = 1\n",
    "\n",
    "# Declate a variable to store the intial y-coordinate of the mid-point of both shoulders of the person.\n",
    "MID_Y = None\n",
    "\n",
    "# Initialize a counter to store count of the number of consecutive frames with person's hands joined.\n",
    "counter = 0\n",
    "\n",
    "# Initialize the number of consecutive frames on which we want to check if person hands joined before starting the game.\n",
    "num_of_frames = 5\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Get the height and width of the frame of the webcam video.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    \n",
    "    # Perform the pose detection on the frame.\n",
    "    frame, results = detectPose(frame, pose_video, draw=game_started)\n",
    "\n",
    "   # cheaking the landmarks in frames\n",
    "    if results.pose_landmarks:\n",
    "\n",
    "        if game_started:\n",
    "            #for horizontal command\n",
    "            frame, horizontal_position =checkLeftRight(frame,results,draw=True)\n",
    "\n",
    "            #move left from center and center from right\n",
    "            if (horizontal_position=='Left' and x_pos_index!=0) or (horizontal_position=='Center' and x_pos_index==2):\n",
    "\n",
    "                #press utokey\n",
    "                ####################################################################\n",
    "                pyautogui.press('left')\n",
    "\n",
    "                #update horizontal\n",
    "                x_pos_index -=1\n",
    "\n",
    "                \n",
    "            elif (horizontal_position=='Right' and x_pos_index!=2) or (horizontal_position=='Center' and x_pos_index==0):\n",
    "                \n",
    "                # Press the right arrow key.\n",
    "                ##################################################################\n",
    "                pyautogui.press('right')\n",
    "                \n",
    "                # Update the horizontal position index of the character.\n",
    "                x_pos_index += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Otherwise if the game has not started    \n",
    "        else:\n",
    "            \n",
    "            # Write the text representing the way to start the game on the frame. \n",
    "            cv2.putText(frame, 'JOIN BOTH HANDS TO START THE GAME.', (5, frame_height - 10), cv2.FONT_HERSHEY_PLAIN,\n",
    "                        2, (0, 255, 0), 3)\n",
    "            \n",
    "         # command to start the game and pause the game\n",
    " \n",
    "        #result ma pose aaraha ha\n",
    "        if checkHandsJoined(frame, results)[1] =='Hands Joined':\n",
    "\n",
    "            counter +=1\n",
    "            if counter==num_of_frames:\n",
    "                # cheak if game not start\n",
    "                if not(game_started):\n",
    "                    game_started=True\n",
    "                    # Retreive the y-coordinate of the left shoulder landmark.\n",
    "                    left_y = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER].y * frame_height)\n",
    "\n",
    "                    # Retreive the y-coordinate of the right shoulder landmark.\n",
    "                    right_y = int(results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER].y * frame_height)\n",
    "\n",
    "                    # Calculate the intial y-coordinate of the mid-point of both shoulders of the person.\n",
    "                    MID_Y = abs(right_y + left_y) // 2\n",
    "\n",
    "                    # Move to 1300, 800, then click the left mouse button to start the game.\n",
    "                    pyautogui.click(x=1300, y=800, button='left')\n",
    "\n",
    "\n",
    "                else:\n",
    "    \n",
    "                    # Press the space key.\n",
    "                    pyautogui.press('space')\n",
    "\n",
    "                counter =0\n",
    "\n",
    "        else:\n",
    "            counter =0\n",
    "        #------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Commands to control the vertical movements of the character.\n",
    "        #------------------------------------------------------------------------------------------------------------------\n",
    "        # cheaking the y coordinate of both solder\n",
    "        if MID_Y:\n",
    "            frame, posture = checkJumpCrouch(frame, results, MID_Y, draw=True)\n",
    "\n",
    "            # for jump\n",
    "            if posture == 'Jumping' and y_pos_index==1:\n",
    "                ###########################################################\n",
    "                pyautogui.press('up')\n",
    "\n",
    "                y_pos_index+=1\n",
    "\n",
    "            elif posture == 'Crouching' and y_pos_index == 1:\n",
    "    \n",
    "                # Press the down arrow key\n",
    "                #########################################################################\n",
    "                pyautogui.press('down')\n",
    "                \n",
    "                # Update the veritcal position index of the character.\n",
    "                y_pos_index -= 1\n",
    "\n",
    "            # Check if the person has stood.\n",
    "            elif posture == 'Standing' and y_pos_index   != 1:\n",
    "                \n",
    "                # Update the veritcal position index of the character.\n",
    "                y_pos_index = 1\n",
    "        \n",
    "        #------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    # Otherwise if the pose landmarks in the frame are not detected.       \n",
    "    else:\n",
    "\n",
    "        # Update the counter value to zero.\n",
    "        counter = 0            \n",
    "\n",
    "\n",
    "    # Calculate the frames updates in one second\n",
    "\n",
    "    #set the time for frame\n",
    "\n",
    "    time2=time()\n",
    "\n",
    "    # Check if the difference between the previous and this frame time > 0 to avoid division by zero.\n",
    "    if (time2 - time1) > 0:\n",
    "    \n",
    "        # Calculate the number of frames per second.\n",
    "        frames_per_second = 1.0 / (time2 - time1)\n",
    "        \n",
    "        # Write the calculated number of frames per second on the frame. \n",
    "        cv2.putText(frame, 'FPS: {}'.format(int(frames_per_second)), (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 3)\n",
    "    \n",
    "    # Update the previous frame time to this frame time.\n",
    "    # As this frame will become previous frame in next iteration.\n",
    "    time1 = time2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Display the frame.            \n",
    "    cv2.imshow('Subway Surfers with Pose Detection', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a a key is pressed, retreive the ASCII code of the key.\n",
    "    k=cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "\n",
    "    if k==ord('x'):\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.\n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:rgb(134,19,348)\"> # Link online playing game\n",
    "https://poki.com/en/g/subway-surfers </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
